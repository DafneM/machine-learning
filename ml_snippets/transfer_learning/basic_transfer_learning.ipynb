{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "from tfa.losses import SigmoidFocalCrossEntropy\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"\"\n",
    "test_data_dir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224,224)\n",
    "BATCH_SIZE = 32\n",
    "seed = np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_kwargs = dict(rescale=1./255, validation_split=.20, \n",
    "    data_format='channels_last')\n",
    "\n",
    "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
    "    interpolation=\"bilinear\", seed=seed)\n",
    "\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    **datagen_kwargs)\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs)\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.05, \n",
    "    height_shift_range=0.2,\n",
    "    **datagen_kwargs)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255, \n",
    "    data_format='channels_last')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir, shuffle=False, target_size=IMAGE_SIZE, batch_size=13,\n",
    "    interpolation=\"bilinear\", seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 Architecture\n",
    "model = Sequential()\n",
    "model.add(keras.applications.vgg16.VGG16(\n",
    "                        include_top=False, \n",
    "                        weights='imagenet', \n",
    "                        input_tensor=None, \n",
    "                        input_shape=(224, 224, 3), \n",
    "#                         pooling='max',\n",
    "                        pooling='avg'))\n",
    "\n",
    "model.add(layers.Flatten(name='flatten'))\n",
    "model.add(layers.Dense(4096, \n",
    "                        activation='relu', \n",
    "                        name='fc1'))\n",
    "model.add(layers.Dense(4096, \n",
    "                        activation='relu', \n",
    "                        name='fc2'))\n",
    "model.add(layers.Dense(4, \n",
    "                        activation='softmax', \n",
    "                        name='predictions'))\n",
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = CategoricalCrossentropy(\n",
    "#     from_logits=False, \n",
    "#     label_smoothing=0, \n",
    "#     name='categorical_crossentropy')\n",
    "loss = SigmoidFocalCrossEntropy(\n",
    "    alpha=0.25,\n",
    "    gamma=2.0,\n",
    "    name='sigmoid_focal_cross_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(\n",
    "    momentum=0.9, \n",
    "    nesterov=True, \n",
    "    name='SGD',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(    \n",
    "    optimizer=optimizer,\n",
    "    loss= loss,\n",
    "    metrics=['accuracy'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scheduler(epoch):\n",
    "#     if epoch == 1:\n",
    "#         return 0.001\n",
    "#     elif epoch == 2:\n",
    "#         return 0.0001\n",
    "#     else:\n",
    "#         return 0.0001 * np.exp(0.1 * (2 - epoch))\n",
    "# learning_rate_scheduler_callback = LearningRateScheduler(scheduler)\n",
    "scheduler = PiecewiseConstantDecay(\n",
    "    boundaries=[150], \n",
    "    values=[0.001, 0.0001]\n",
    ")\n",
    "learning_rate_scheduler_callback = LearningRateScheduler(scheduler)\n",
    "callbacks.append(learning_rate_scheduler_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    os.path.join('models','output_models','weights_only_bla_bla_bla_{epoch:02d}_{val_loss:.4f}.hdf5'), \n",
    "#     monitor='val_loss', \n",
    "    verbose=0, \n",
    "    save_best_only=False,\n",
    "    save_weights_only=True, \n",
    "    mode='auto', \n",
    "    save_freq='epoch'\n",
    ")\n",
    "callbacks.append(model_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weight = class_weight.compute_class_weight(\n",
    "#     'balanced', \n",
    "#     np.unique(train_generator.classes), \n",
    "#     train_generator.classes)\n",
    "# class_weight = dict(zip(dict(valid_generator.class_indices).values(), class_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
    "hist = model.fit(\n",
    "    train_generator,\n",
    "    epochs=1, \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=validation_steps,\n",
    "#     class_weight=class_weight,\n",
    "    callbacks=callbacks\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = test_generator.n//test_generator.batch_size\n",
    "Y_pred = model.predict(test_generator, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,2])\n",
    "plt.plot(hist[\"loss\"])\n",
    "plt.plot(hist[\"val_loss\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,1])\n",
    "plt.plot(hist[\"accuracy\"])\n",
    "plt.plot(hist[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
